{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfb2a24-05ab-4b1d-8fc2-7448458663bd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=goodplace2.png align=right width=500>\n",
    "\n",
    "The TV show *The Good Place* is centered around a number of humans who have died and find themselves in the afterlife.  In this conception\n",
    "of the afterlife, humans are sent to \"the Good Place\" or \"the Bad Place\" after death.  All humans are assigned a numerical score based on the morality of their conduct in life, and only those with the very highest scores are sent to the \"Good Place\", where they enjoy eternal happiness; all others experience an eternity of torture in the \"Bad Place.\"\n",
    "\n",
    "In this project, you will explore using logistic regression to predict whether someone will end up in the \"Good Place or the \"Bad Place\" based on an\n",
    "extremely scaled down version of their conduct in life.  In particular, we have data for 1000 people about how often they:\n",
    "\n",
    "- Let someone merge in front of them in traffic\n",
    "- Didn't tip their server at a restaurant\n",
    "- Held a door open for someone who was walking behind them\n",
    "- Littered\n",
    "\n",
    "These will be our four features for the problem.  Our data set consists of these four features tallied for 1000 different people.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a37aed2-9e9c-43c6-a7fb-0d93fddff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10cb214-0b0d-4fbc-b6e9-38a4b4e066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"data1.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_csv(\"data1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ce96-70c8-4ae6-aecf-2d06692fe2e8",
   "metadata": {},
   "source": [
    "## Explanation of the data file\n",
    "\n",
    "Each row of the file represents data about a person.  \n",
    "\n",
    "The first four columns should be self-explanatory: they tell how often a person did a\n",
    "certain activity (explained above).  There are two columns at the end saying whether they\n",
    "ended up in the \"Good Place\" or the \"Bad Place.\"\n",
    "\n",
    "The first of the two (`goodbad`) is calculated \"perfectly\" from a formula I came up with (that I'm keeping secret!)\n",
    "\"Perfectly\" meaning that the formula itself probably isn't perfect, but the good/bad column is calculated\n",
    "directly mathematically from the formula, based on the four features.\n",
    "\n",
    "The second of the two (`noisygoodbad`) is also calculated perfectly from the formula, but\n",
    "with some \"noise\" thrown in.  In other words, I've switched a few of the goods to bads and vice versa, to\n",
    "simulate a real-world situation (where the Good Place/Bad Place determination is based not only on this\n",
    "data, but other data as well that we don't have access to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3786e4-a047-4030-bf20-e97b083b15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few and last few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 1000\n",
    "df  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddc7dab4-a3d6-4f4d-83f0-60219edab309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered\n",
       "0          13      0       102         7\n",
       "1          24     40       295       224\n",
       "2          42      8       356       182\n",
       "3         194     20       485       193\n",
       "4         196     24       376       175\n",
       "..        ...    ...       ...       ...\n",
       "995        37      3       150       137\n",
       "996        15     17       439       234\n",
       "997        67     22       480       113\n",
       "998        57     20       470        54\n",
       "999       141      4       447        74\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select our X and y data\n",
    "\n",
    "# First, we will split the data frame above into a four-column frame\n",
    "# with the input features (X's) and a one-column frame with the target\n",
    "# feature (y), which we will use the noisy column (noisygoodbad).\n",
    "\n",
    "# Write code below to create df_X with just the four X feature columns,\n",
    "# and df_y that has just the noisygoodbad column.\n",
    "\n",
    "# Then **normalize** the X values with Z-score normalization as in \n",
    "# project 1.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df_X = df[['letMerge', 'noTip', 'heldDoor', 'littered']]\n",
    "df_y = df['noisygoodbad']\n",
    "\n",
    "# Sanity check for df_X\n",
    "\n",
    "df_X  # Should print a data frame with 1000 rows and 4 columns.\n",
    "# First row should be [ -1.161413 -1.267048 -1.350553 -1.620752]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f31299c-bed6-44be-bd11-bb5eb84ba006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      good\n",
       "1       bad\n",
       "2      good\n",
       "3      good\n",
       "4      good\n",
       "       ... \n",
       "995     bad\n",
       "996     bad\n",
       "997    good\n",
       "998    good\n",
       "999    good\n",
       "Name: noisygoodbad, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_y\n",
    "\n",
    "df_y  # Should be a column of goods and bads, starting with good, bad, good, good, good...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6667ab3c-7106-4b57-b612-4756034136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    1\n",
       "998    1\n",
       "999    1\n",
       "Name: noisygoodbad, Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to switch df_y to not have good/bad strings, but rather 0's and 1's.\n",
    "# Use this line of code:\n",
    "\n",
    "df_y = (df_y == 'good').astype(int)\n",
    "\n",
    "# Sanity check: should now be a column of ones and zeros, with 1=good, 0=bad.  \n",
    "df_y   # Should begin 1, 0, 1, 1, 1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6d6faa7-5308-4884-ac49-ed5af1eb15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check shapes:\n",
    "\n",
    "print(df_X.shape) # Should be (1000, 4)\n",
    "print(df_y.shape) # Should be (1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7ea4ac9-0c3c-470b-8425-2da67ca44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "\n",
    "# We want to write code to split the data frame above into a few\n",
    "# new data frames.  In particular, are going to have a TRAINING SET\n",
    "# and a TESTING SET for this project.  We will use 80% of the data for \n",
    "# training, and the remaining 20% for testing.  \n",
    "\n",
    "# In the real world, we would split the data randomly, but so we all\n",
    "# end up with the same results, we will use the first 80% of the data\n",
    "# for training, and the last 20% for testing (in order of how the rows\n",
    "# show up in the file).  Note that there are 1000 people (rows in \n",
    "# the file), so the first 800 rows will be training, and the last 200\n",
    "# will be testing.\n",
    "\n",
    "# Write code here to create FOUR NUMPY ndarrays:\n",
    "\n",
    "# - X_train: first 800 lines of df_X\n",
    "# - X_test: last 200 lines of df_X\n",
    "# - y_train: first 800 lines of df_y\n",
    "# - y_test: last 200 lines of df_y\n",
    "X_train_start = np.array(df_X.head(800))\n",
    "X_test_start = np.array(df_X.tail(200))\n",
    "\n",
    "y_train = np.array(df_y.head(800))\n",
    "y_test = np.array(df_y.tail(200))\n",
    "\n",
    "\n",
    "# Then, add a column of ones to the left side of X_train and X_test.\n",
    "X_train = np.hstack((np.ones((800, 1)), X_train_start))\n",
    "X_test = np.hstack((np.ones((200, 1)), X_test_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a3f59aa-ba82-4ec5-9234-b390140e4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(800,)\n",
      "(200, 5)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks:\n",
    "\n",
    "print(X_train.shape) # Should be (800, 5)\n",
    "print(y_train.shape) # Should be (800,) \n",
    "print(X_test.shape) # Should be (200, 5)\n",
    "print(y_test.shape) # Should be (200,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "129456c3-f616-41bd-a93e-79d585030e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training examples:\n",
      "[[  1.  13.   0. 102.   7.   1.]\n",
      " [  1.  24.  40. 295. 224.   0.]\n",
      " [  1.  42.   8. 356. 182.   1.]\n",
      " [  1. 194.  20. 485. 193.   1.]\n",
      " [  1. 196.  24. 376. 175.   1.]\n",
      " [  1. 135.  21. 236. 141.   0.]\n",
      " [  1. 126.   3. 376.  31.   1.]\n",
      " [  1.  32.   0. 416.  67.   1.]\n",
      " [  1. 240.   6. 179.  88.   1.]\n",
      " [  1.   0.  29. 191.  86.   0.]]\n",
      "\n",
      "First 10 testing examples:\n",
      "[[  1. 118.   6. 284. 166.   1.]\n",
      " [  1.  31.  10. 227. 129.   0.]\n",
      " [  1. 157.  31. 173. 176.   0.]\n",
      " [  1.  69.   9. 102. 130.   0.]\n",
      " [  1.  40.  24.  71. 207.   0.]\n",
      " [  1. 139.   0. 330.  52.   1.]\n",
      " [  1. 163.  32. 167.  43.   1.]\n",
      " [  1.   8.   5. 348.  87.   1.]\n",
      " [  1.  77.   4.  62. 175.   0.]\n",
      " [  1. 118.   1. 186. 210.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows of training/testing data:  (will be useful to have these later)\n",
    "\n",
    "print(\"First 10 training examples:\")\n",
    "print(np.hstack([X_train, y_train.reshape(-1, 1)])[0:10])\n",
    "print()\n",
    "print(\"First 10 testing examples:\")\n",
    "print(np.hstack([X_test, y_test.reshape(-1, 1)])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f575209-8473-461d-99d3-51e482549cc7",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Like in Part A of the previous project, we will rely on an external method to create\n",
    "a logistic regression model for us, then we will see if we can replicate it ourselves.\n",
    "\n",
    "Below is code that uses scikit-learn to do this for us.  Don't worry too much about what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "102fdfea-65d7-4803-9ea4-c2fa5f56e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\grant\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\grant\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\grant\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\grant\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\grant\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "w found through scikit-learn: [-0.74210242  0.10016553 -0.63075989  0.06833209 -0.13407739]\n"
     ]
    }
   ],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, penalty=None, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "w_direct = model.coef_[0]\n",
    "\n",
    "print(\"w found through scikit-learn:\", w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e005-4294-4c93-a8a7-227c6db31309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, write a sentence about how to interpret these \n",
    "# numbers in w_direct, in particular, (1) why are some negative\n",
    "# and some positive, and (2) what is the special interpretation of\n",
    "# w_direct[0]?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "# 1) Some of these values are negative because they are an under apporximation of what the \n",
    "#    accepted value is supposed to be.\n",
    "#\n",
    "# 2) w_direct[0] should be 1 because the first collum is made entirely of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031723e-d89c-4613-8eb7-a8bd7cd16b64",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In this part you will write code for binary logistic regression by hand, including the model,\n",
    "the loss function, the cost function, and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79ea1eb0-3006-4789-a7da-50bd839697f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid function\n",
    "\n",
    "# Write code here to define the sigmoid function 1/(1+e^-x).  \n",
    "# IMPORTANT: Use the np.exp function to raise e to a power.  \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0d7adf2-20fe-4f18-a59f-863ffaf8f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.2689414213699951 0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(sigmoid(0), sigmoid(-1), sigmoid(0.5))\n",
    "\n",
    "# should print 0.5 0.2689414213699951 0.6224593312018546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb4f2961-7789-4ff7-8aec-35e140601a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called run_model below to run the logistic\n",
    "# regression model on one feature vector (x_data).\n",
    "# In other words, this function should compute 1/(1 + e^(-wx))\n",
    "# where x is x_data.  But do this by calling your sigmoid function\n",
    "# and the dot product function (np.dot()).\n",
    "\n",
    "def run_model(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    wx = np.dot(w, x_data)\n",
    "    \n",
    "    # Compute scarlar using the sigmoid function\n",
    "    scalar = sigmoid(wx)\n",
    "    \n",
    "    return scalar\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d36a583-5503-4b8b-ace8-851d3b0e5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998629735827999"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: run the model from Part A on the first testing example\n",
    "\n",
    "run_model(w_direct, X_train[0])  # should be 0.9986297185506662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "111adf69-bbe2-4bab-a09e-161deb3aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# The run_model function only returns numbers in a certain range.  What is this range\n",
    "# and why does this function not return numbers outside of that range?\n",
    "\n",
    "# ANSWER:\n",
    "# run_model returns a value between [0, 1]. These are probability representations. \n",
    "# Thus, 1 = 100% || 0 = 0%\n",
    "# Any value given above 1 would be statistically imposible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5638376f-586d-4fdc-bb96-4a1d4d322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called make_prediction that will\n",
    "# actually predict the class 0 or 1 for a feature vector x_data.\n",
    "# To do this, just call run_model and check if the return\n",
    "# value is > or < than 0.5\n",
    "\n",
    "def make_prediction(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    wx = np.dot(w, x_data)\n",
    "    \n",
    "    # Compute scarlar using the sigmoid function\n",
    "    prediction = sigmoid(wx)\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7e4e0e9-0efc-46c1-a012-2da1a7b05ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for test example 0 -> 1\n",
      "Predicted class for test example 1 -> 0\n",
      "Predicted class for test example 2 -> 0\n",
      "Predicted class for test example 3 -> 0\n",
      "Predicted class for test example 4 -> 0\n",
      "Predicted class for test example 5 -> 1\n",
      "Predicted class for test example 6 -> 1\n",
      "Predicted class for test example 7 -> 1\n",
      "Predicted class for test example 8 -> 0\n",
      "Predicted class for test example 9 -> 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: classify the first few testing examples using the model from Part A\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Predicted class for test example\", i, \"->\", make_prediction(X_test[i], w_direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3309fc50-bf06-4b31-9740-6f13f1b5eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Given the output immediately above, what is the accuracy of the model in Part A (since we used w_direct\n",
    "# above) just based on these 10 training examples?  (Answer as a percent; in other words\n",
    "# the percentage of those 10 testing examples that were predicted correctly).\n",
    "\n",
    "# ANSWER:\n",
    "# 4 out of 10 of the first 10 testing examples were predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56291ae2-12bf-498b-9500-df3f1d66a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called compute_accuracy that takes a \n",
    "# set of X values and a set of y values and a parameter vector\n",
    "# w.  This function should predict the class for each example x\n",
    "# in X_data and based on the true y values (y_data), compute\n",
    "# the accuracy on this data set.\n",
    "\n",
    "# To do this, call make_prediction on each row of X_data\n",
    "# and compare the output against the corresponding value in y_data.\n",
    "# Count how many predictions are correct and divide by the total.\n",
    "\n",
    "def compute_accuracy(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix of features (flexible rows, n+1 cols)\n",
    "    y_data: vector of true classes (same number of rows as X_data)\n",
    "    w: array of weights (n+1)\n",
    "    returns: percentage of rows in X_data classified correctly\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    samples = X_data.shape[0]\n",
    "    correct_counter = 0\n",
    "    for i in range(samples):\n",
    "        y_pred = make_prediction(X_data[i], w)\n",
    "        \n",
    "        if y_pred == y_data[i]:\n",
    "            correct_counter += 1\n",
    "    \n",
    "    accuracy = correct_counter / samples\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bbcece4e-4d1c-4627-ac5d-bf70f9b9b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "train_acc_partA = compute_accuracy(X_train, y_train, w_direct)\n",
    "test_acc_partA = compute_accuracy(X_test, y_test, w_direct)\n",
    "\n",
    "print(train_acc_partA)  # should be 0.9625\n",
    "print(test_acc_partA)  # should be 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff4b3493-862b-42f9-b0cc-5dfa761729fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Which of the two numbers above do we report as the \"true\" accuracy of our model,\n",
    "# and why do we typically not report the other (or not give it as much importance)?\n",
    "\n",
    "# ANSWER:\n",
    "# The 'true' accuracy value is test_acc_partA. Becasue reporting an accuracy value for the training data set doesnt tell me \n",
    "# much about the actuall accuracy of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f7840f3-d429-4302-baa9-5d90c1435740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_loss function below to compute the\n",
    "# loss over *one* training example, given the true y value\n",
    "# and the predicted y value (y_hat).  \n",
    "# This is the logistic regression loss function.\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    y: 0 or 1 \n",
    "    y_hat: decimal number between 0 and 1\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # ùêø(ùë¶ÃÇ ,ùë¶) = ‚àí(ùë¶ log(ùë¶ÃÇ )) ‚àí ((1‚àíùë¶) log(1‚àíùë¶ÃÇ )))\n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)\n",
    "    elif y == 0:\n",
    "        return -np.log(1-y_hat)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b9d6a2e2-9c16-45fa-a359-0f83a9d3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w.\n",
    "# Call your run_model() and compute_loss() functions \n",
    "# that you defined above.  You should have one loop.\n",
    "# DO NOT CALL MAKE_PREDICTION; it's not needed here.\n",
    "\n",
    "def compute_cost(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    total_runs = X_data.shape[0] # number of examples\n",
    "    \n",
    "    total_cost = 0\n",
    "    for i in range(0, total_runs):\n",
    "        x_i = X_data[i]\n",
    "        y_i = y_data[i]\n",
    "        total_cost = total_cost + compute_loss(y_i, run_model(x_i, w))\n",
    "    \n",
    "    return (total_cost / 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40f04248-f046-4865-9246-9d27d63afd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058819782685208585\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compute the loss for w_direct from Part A:\n",
    "\n",
    "w_direct_cost = compute_cost(X_train, y_train, w_direct)  # This is the minimum cost we can ever get!  Should be less than 0.1.\n",
    "print(w_direct_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ca88aeed-7b1a-40fc-a742-ba127c2a670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# compute_cost() above returns a single number, but based on the formula in J(w),\n",
    "# this function can only ever return numbers in a fixed range.  What is that range\n",
    "# and why are only numbers in this range ever returned?\n",
    "\n",
    "# ANSWER:\n",
    "# compute_cost() returns a value between [0, inf]. Compute Loss is unbounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed2c1a63-7376-41db-8235-cfee2ef8dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(w).  \n",
    "# Do not use matrix computations here; call your run_model() function\n",
    "# that you defined above.  You should have two nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    m = len(X_data)\n",
    "    gradient = np.zeros(w.shape)\n",
    "    \n",
    "    for i in range(m): \n",
    "        y_hat = make_prediction(X_data[i], w) \n",
    "        cost = (y_hat - y_data[i])\n",
    "        \n",
    "        for j in range(len(w)):\n",
    "            gradient[j] = gradient[j] + (X_data[i][j] * cost)\n",
    "            \n",
    "    return gradient / m\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7645ea34-7117-4971-abca-4b7084038f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Current w_manual: [0. 0. 0. 0. 0.]\n",
      "The cost is: 0.5545177444479541\n",
      "\n",
      "Iteration: 1\n",
      "Current w_manual: [-4.187500e-09 -2.752000e-07 -7.080000e-08 -8.888375e-07 -6.907250e-07]\n",
      "The cost is: 0.5545536411198468\n",
      "\n",
      "Iteration: 2\n",
      "Current w_manual: [ 1.6250000e-09  3.2768750e-07 -1.5275000e-08  1.0237375e-06\n",
      " -1.4765000e-07]\n",
      "The cost is: 0.5544706569105282\n",
      "\n",
      "Iteration: 3\n",
      "Current w_manual: [-2.56250e-09  5.24875e-08 -8.60750e-08  1.34900e-07 -8.38375e-07]\n",
      "The cost is: 0.5545065292955482\n",
      "\n",
      "Iteration: 4\n",
      "Current w_manual: [ 1.262500e-09  4.791000e-07 -5.400000e-08  1.389550e-06 -3.499375e-07]\n",
      "The cost is: 0.5544524818466832\n",
      "\n",
      "Iteration: 5\n",
      "Current w_manual: [-2.912500e-09  2.040250e-07 -1.245250e-07  5.013750e-07 -1.037575e-06]\n",
      "The cost is: 0.5544883364176642\n",
      "\n",
      "Iteration: 6\n",
      "Current w_manual: [-3.2250000e-09  2.5293750e-07 -1.4710000e-07  3.6940000e-07\n",
      " -1.0296375e-06]\n",
      "The cost is: 0.5544931324282613\n",
      "\n",
      "Iteration: 7\n",
      "Current w_manual: [-2.262500e-09  4.031875e-07 -1.517625e-07  6.669250e-07 -8.229000e-07]\n",
      "The cost is: 0.554480199377344\n",
      "\n",
      "Iteration: 8\n",
      "Current w_manual: [-4.9750000e-09  2.1916250e-07 -2.0256250e-07 -4.6762500e-08\n",
      " -1.2345625e-06]\n",
      "The cost is: 0.5545093749077219\n",
      "\n",
      "Iteration: 9\n",
      "Current w_manual: [ 5.50000e-10  7.72975e-07 -1.50000e-07  1.79510e-06 -6.94300e-07]\n",
      "The cost is: 0.5544299412255705\n",
      "\n",
      "Iteration: 10\n",
      "Current w_manual: [-3.6000000e-09  4.9797500e-07 -2.2006250e-07  9.0853750e-07\n",
      " -1.3765375e-06]\n",
      "The cost is: 0.5544657531027692\n",
      "\n",
      "Iteration: 11\n",
      "Current w_manual: [-5.512500e-09  3.951000e-07 -2.604375e-07  3.512875e-07 -1.641350e-06]\n",
      "The cost is: 0.5544883302330644\n",
      "\n",
      "Iteration: 12\n",
      "Current w_manual: [-2.937500e-09  6.919500e-07 -2.450750e-07  1.211275e-06 -1.244375e-06]\n",
      "The cost is: 0.5544515841827028\n",
      "\n",
      "Iteration: 13\n",
      "Current w_manual: [-5.9500000e-09  4.9052500e-07 -2.9977500e-07  4.4773750e-07\n",
      " -1.7094625e-06]\n",
      "The cost is: 0.5544827035600334\n",
      "\n",
      "Iteration: 14\n",
      "Current w_manual: [-4.1375000e-09  7.0610000e-07 -2.9356250e-07  1.0433000e-06\n",
      " -1.3956125e-06]\n",
      "The cost is: 0.554457351279444\n",
      "\n",
      "Iteration: 15\n",
      "Current w_manual: [-6.700000e-09  5.320875e-07 -3.418750e-07  3.601125e-07 -1.779175e-06]\n",
      "The cost is: 0.5544853099624912\n",
      "\n",
      "Iteration: 16\n",
      "Current w_manual: [-4.1250000e-09  8.2170000e-07 -3.2593750e-07  1.2282000e-06\n",
      " -1.3826375e-06]\n",
      "The cost is: 0.5544483247432673\n",
      "\n",
      "Iteration: 17\n",
      "Current w_manual: [-7.000000e-09  6.277500e-07 -3.785375e-07  4.894875e-07 -1.820725e-06]\n",
      "The cost is: 0.5544784903183408\n",
      "\n",
      "Iteration: 18\n",
      "Current w_manual: [-5.387500e-09  8.184875e-07 -3.743000e-07  1.020175e-06 -1.531775e-06]\n",
      "The cost is: 0.5544559713869841\n",
      "\n",
      "Iteration: 19\n",
      "Current w_manual: [-7.700000e-09  6.632000e-07 -4.189750e-07  3.959250e-07 -1.864725e-06]\n",
      "The cost is: 0.5544815727204407\n",
      "\n",
      "Iteration: 20\n",
      "Current w_manual: [-5.5250000e-09  8.9786250e-07 -4.0787500e-07  1.1263875e-06\n",
      " -1.5134250e-06]\n",
      "The cost is: 0.5544506713193093\n",
      "\n",
      "Iteration: 21\n",
      "Current w_manual: [-8.150000e-09  7.157875e-07 -4.563500e-07  4.352000e-07 -1.907275e-06]\n",
      "The cost is: 0.5544790010764342\n",
      "\n",
      "Iteration: 22\n",
      "Current w_manual: [-6.187500e-09  9.301250e-07 -4.474625e-07  1.099200e-06 -1.577625e-06]\n",
      "The cost is: 0.5544509579486517\n",
      "\n",
      "Iteration: 23\n",
      "Current w_manual: [-8.6750000e-09  7.5528750e-07 -4.9362500e-07  4.3842500e-07\n",
      " -1.9460625e-06]\n",
      "The cost is: 0.5544780995607473\n",
      "\n",
      "Iteration: 24\n",
      "Current w_manual: [-6.7375000e-09  9.6513750e-07 -4.8491250e-07  1.0995250e-06\n",
      " -1.6187375e-06]\n",
      "The cost is: 0.5544502200617698\n",
      "\n",
      "Iteration: 25\n",
      "Current w_manual: [-9.2000000e-09  7.9267500e-07 -5.3103750e-07  4.4310000e-07\n",
      " -1.9827875e-06]\n",
      "The cost is: 0.5544771784494257\n",
      "\n",
      "Iteration: 26\n",
      "Current w_manual: [-7.3000000e-09  9.9405000e-07 -5.2268750e-07  1.0974375e-06\n",
      " -1.6601875e-06]\n",
      "The cost is: 0.5544496586751344\n",
      "\n",
      "Iteration: 27\n",
      "Current w_manual: [-9.650000e-09  8.299500e-07 -5.675625e-07  4.691250e-07 -2.000775e-06]\n",
      "The cost is: 0.5544754956516167\n",
      "\n",
      "Iteration: 28\n",
      "Current w_manual: [-7.8750000e-09  1.0148625e-06 -5.6066250e-07  1.0854500e-06\n",
      " -1.6932125e-06]\n",
      "The cost is: 0.5544496583560911\n",
      "\n",
      "Iteration: 29\n",
      "Current w_manual: [-1.016250e-08  8.547875e-07 -6.050125e-07  4.715375e-07 -2.022175e-06]\n",
      "The cost is: 0.554474922132736\n",
      "\n",
      "Iteration: 30\n",
      "Current w_manual: [-8.4125000e-09  1.0344875e-06 -5.9848750e-07  1.0825000e-06\n",
      " -1.7179375e-06]\n",
      "The cost is: 0.5544493527761609\n",
      "\n",
      "Iteration: 31\n",
      "Current w_manual: [-1.0662500e-08  8.7703750e-07 -6.4231250e-07  4.7842500e-07\n",
      " -2.0392375e-06]\n",
      "The cost is: 0.5544742249711057\n",
      "\n",
      "Iteration: 32\n",
      "Current w_manual: [-8.9250000e-09  1.0557625e-06 -6.3616250e-07  1.0856500e-06\n",
      " -1.7361750e-06]\n",
      "The cost is: 0.5544488142884267\n",
      "\n",
      "Iteration: 33\n",
      "Current w_manual: [-1.117500e-08  8.983125e-07 -6.799875e-07  4.815750e-07 -2.057475e-06]\n",
      "The cost is: 0.5544736864791318\n",
      "\n",
      "Iteration: 34\n",
      "Current w_manual: [-9.4625000e-09  1.0719875e-06 -6.7415000e-07  1.0813000e-06\n",
      " -1.7582625e-06]\n",
      "The cost is: 0.5544486260589969\n",
      "\n",
      "Iteration: 35\n",
      "Current w_manual: [-1.167500e-08  9.171750e-07 -7.179375e-07  4.853500e-07 -2.072925e-06]\n",
      "The cost is: 0.5544731703011214\n",
      "\n",
      "Iteration: 36\n",
      "Current w_manual: [-9.975000e-09  1.089650e-06 -7.121000e-07  1.079475e-06 -1.775550e-06]\n",
      "The cost is: 0.5544483440124922\n",
      "\n",
      "Iteration: 37\n",
      "Current w_manual: [-1.216250e-08  9.361625e-07 -7.551500e-07  4.891500e-07 -2.086250e-06]\n",
      "The cost is: 0.5544726644544195\n",
      "\n",
      "Iteration: 38\n",
      "Current w_manual: [-1.0475000e-08  1.1062125e-06 -7.4956250e-07  1.0772125e-06\n",
      " -1.7912875e-06]\n",
      "The cost is: 0.5544481037459599\n",
      "\n",
      "Iteration: 39\n",
      "Current w_manual: [-1.2625000e-08  9.5461250e-07 -7.9192500e-07  4.9633750e-07\n",
      " -2.0953875e-06]\n",
      "The cost is: 0.5544720519176584\n",
      "\n",
      "Iteration: 40\n",
      "Current w_manual: [-1.101250e-08  1.116925e-06 -7.871750e-07  1.059025e-06 -1.809575e-06]\n",
      "The cost is: 0.554448576899063\n",
      "\n",
      "Iteration: 41\n",
      "Current w_manual: [-1.303750e-08  9.726500e-07 -8.278875e-07  5.109000e-07 -2.090475e-06]\n",
      "The cost is: 0.55447122621041\n",
      "\n",
      "Iteration: 42\n",
      "Current w_manual: [-1.1500000e-08  1.1265000e-06 -8.2383750e-07  1.0470625e-06\n",
      " -1.8146500e-06]\n",
      "The cost is: 0.5544488887714042\n",
      "\n",
      "Iteration: 43\n",
      "Current w_manual: [-1.3512500e-08  9.8257500e-07 -8.6428750e-07  5.0161250e-07\n",
      " -2.0938875e-06]\n",
      "The cost is: 0.5544714340934475\n",
      "\n",
      "Iteration: 44\n",
      "Current w_manual: [-1.1937500e-08  1.1409875e-06 -8.5988750e-07  1.0513125e-06\n",
      " -1.8128000e-06]\n",
      "The cost is: 0.5544485138647185\n",
      "\n",
      "Iteration: 45\n",
      "Current w_manual: [-1.396250e-08  9.949125e-07 -9.006000e-07  5.046250e-07 -2.093975e-06]\n",
      "The cost is: 0.5544711263482316\n",
      "\n",
      "Iteration: 46\n",
      "Current w_manual: [-1.2425000e-08  1.1487625e-06 -8.9655000e-07  1.0407875e-06\n",
      " -1.8181500e-06]\n",
      "The cost is: 0.5544487888538219\n",
      "\n",
      "Iteration: 47\n",
      "Current w_manual: [-1.441250e-08  1.005625e-06 -9.362500e-07  5.005875e-07 -2.094225e-06]\n",
      "The cost is: 0.5544711280461828\n",
      "\n",
      "Iteration: 48\n",
      "Current w_manual: [-1.2862500e-08  1.1615750e-06 -9.3213750e-07  1.0425250e-06\n",
      " -1.8160375e-06]\n",
      "The cost is: 0.5544485405838256\n",
      "\n",
      "Iteration: 49\n",
      "Current w_manual: [-1.4862500e-08  1.0162875e-06 -9.7210000e-07  5.0108750e-07\n",
      " -2.0940500e-06]\n",
      "The cost is: 0.5544709469753969\n",
      "\n",
      "Iteration: 50\n",
      "Current w_manual: [-1.3325000e-08  1.1701375e-06 -9.6805000e-07  1.0372500e-06\n",
      " -1.8182250e-06]\n",
      "The cost is: 0.5544486094757108\n",
      "\n",
      "Iteration: 51\n",
      "Current w_manual: [-1.5325000e-08  1.0248500e-06 -1.0080125e-06  4.9581250e-07\n",
      " -2.0962375e-06]\n",
      "The cost is: 0.5544710159342686\n",
      "\n",
      "Iteration: 52\n",
      "Current w_manual: [-1.3737500e-08  1.1823000e-06 -1.0033000e-06  1.0501625e-06\n",
      " -1.8146500e-06]\n",
      "The cost is: 0.5544479210978703\n",
      "\n",
      "Iteration: 53\n",
      "Current w_manual: [-1.5762500e-08  1.0352000e-06 -1.0435875e-06  5.0331250e-07\n",
      " -2.0967875e-06]\n",
      "The cost is: 0.5544705481971585\n",
      "\n",
      "Iteration: 54\n",
      "Current w_manual: [-1.4262500e-08  1.1840000e-06 -1.0400625e-06  1.0295000e-06\n",
      " -1.8255750e-06]\n",
      "The cost is: 0.5544486576984679\n",
      "\n",
      "Iteration: 55\n",
      "Current w_manual: [-1.622500e-08  1.040075e-06 -1.079300e-06  4.995250e-07 -2.096575e-06]\n",
      "The cost is: 0.5544706188654761\n",
      "\n",
      "Iteration: 56\n",
      "Current w_manual: [-1.4675000e-08  1.1935375e-06 -1.0747750e-06  1.0406500e-06\n",
      " -1.8199875e-06]\n",
      "The cost is: 0.5544480880855636\n",
      "\n",
      "Iteration: 57\n",
      "Current w_manual: [-1.6662500e-08  1.0488000e-06 -1.1143875e-06  5.0346250e-07\n",
      " -2.0954125e-06]\n",
      "The cost is: 0.5544703287194288\n",
      "\n",
      "Iteration: 58\n",
      "Current w_manual: [-1.5150000e-08  1.1977875e-06 -1.1106500e-06  1.0338375e-06\n",
      " -1.8232125e-06]\n",
      "The cost is: 0.554448270323566\n",
      "\n",
      "Iteration: 59\n",
      "Current w_manual: [-1.7112500e-08  1.0538625e-06 -1.1498875e-06  5.0386250e-07\n",
      " -2.0942125e-06]\n",
      "The cost is: 0.5544702314102502\n",
      "\n",
      "Iteration: 60\n",
      "Current w_manual: [-1.5600000e-08  1.2028500e-06 -1.1461500e-06  1.0342375e-06\n",
      " -1.8220125e-06]\n",
      "The cost is: 0.5544481730257496\n",
      "\n",
      "Iteration: 61\n",
      "Current w_manual: [-1.7562500e-08  1.0589250e-06 -1.1853875e-06  5.0426250e-07\n",
      " -2.0930125e-06]\n",
      "The cost is: 0.5544701341011404\n",
      "\n",
      "Iteration: 62\n",
      "Current w_manual: [-1.6050000e-08  1.2079125e-06 -1.1816500e-06  1.0346375e-06\n",
      " -1.8208125e-06]\n",
      "The cost is: 0.5544480757279979\n",
      "\n",
      "Iteration: 63\n",
      "Current w_manual: [-1.8012500e-08  1.0639875e-06 -1.2208875e-06  5.0466250e-07\n",
      " -2.0918125e-06]\n",
      "The cost is: 0.5544700367920942\n",
      "\n",
      "Iteration: 64\n",
      "Current w_manual: [-1.6487500e-08  1.2134750e-06 -1.2168875e-06  1.0399000e-06\n",
      " -1.8183375e-06]\n",
      "The cost is: 0.5544477805138383\n",
      "\n",
      "Iteration: 65\n",
      "Current w_manual: [-1.8450000e-08  1.0695500e-06 -1.2561250e-06  5.0992500e-07\n",
      " -2.0893375e-06]\n",
      "The cost is: 0.5544697414967187\n",
      "\n",
      "Iteration: 66\n",
      "Current w_manual: [-1.7000000e-08  1.2121875e-06 -1.2528500e-06  1.0223125e-06\n",
      " -1.8244750e-06]\n",
      "The cost is: 0.5544484591574892\n",
      "\n",
      "Iteration: 67\n",
      "Current w_manual: [-1.8925000e-08  1.0696750e-06 -1.2913250e-06  5.0297500e-07\n",
      " -2.0890125e-06]\n",
      "The cost is: 0.5544700049066872\n",
      "\n",
      "Iteration: 68\n",
      "Current w_manual: [-1.7400000e-08  1.2186375e-06 -1.2869500e-06  1.0393750e-06\n",
      " -1.8157500e-06]\n",
      "The cost is: 0.5544477068699518\n",
      "\n",
      "Iteration: 69\n",
      "Current w_manual: [-1.9387500e-08  1.0712000e-06 -1.3262125e-06  5.0345000e-07\n",
      " -2.0924875e-06]\n",
      "The cost is: 0.5544699236728946\n",
      "\n",
      "Iteration: 70\n",
      "Current w_manual: [-1.7825000e-08  1.2236000e-06 -1.3214000e-06  1.0466625e-06\n",
      " -1.8161000e-06]\n",
      "The cost is: 0.5544473204118411\n",
      "\n",
      "Iteration: 71\n",
      "Current w_manual: [-1.9837500e-08  1.0735125e-06 -1.3609625e-06  5.0405000e-07\n",
      " -2.0982375e-06]\n",
      "The cost is: 0.5544698136525485\n",
      "\n",
      "Iteration: 72\n",
      "Current w_manual: [-1.8275000e-08  1.2259125e-06 -1.3561500e-06  1.0472625e-06\n",
      " -1.8218500e-06]\n",
      "The cost is: 0.5544472103601972\n",
      "\n",
      "Iteration: 73\n",
      "Current w_manual: [-2.0287500e-08  1.0758250e-06 -1.3957125e-06  5.0465000e-07\n",
      " -2.1039875e-06]\n",
      "The cost is: 0.5544697036323893\n",
      "\n",
      "Iteration: 74\n",
      "Current w_manual: [-1.8712500e-08  1.2306750e-06 -1.3906000e-06  1.0525625e-06\n",
      " -1.8254125e-06]\n",
      "The cost is: 0.5544468889068895\n",
      "\n",
      "Final w_manual: [-2.0725000e-08  1.0805875e-06 -1.4301625e-06  5.0995000e-07\n",
      " -2.1075500e-06]\n",
      "Final w_manual_cost: 0.5544468889068895\n"
     ]
    }
   ],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "w_manual = np.zeros(X_train.shape[1])  # n+1 weights\n",
    "w_manual_cost = 0\n",
    "J_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ALPHA = .00000001\n",
    "\n",
    "for i in range(0, 75):\n",
    "    print(\"Iteration:\", i)\n",
    "    print(\"Current w_manual:\", w_manual)\n",
    "    w_manual_cost = compute_cost(X_train, y_train, w_manual)\n",
    "    print(\"The cost is:\", w_manual_cost)\n",
    "    print()\n",
    "    \n",
    "    J_list.append(w_manual_cost)\n",
    "    \n",
    "    gradient_w = compute_gradient(X_train, y_train, w_manual)\n",
    "    w_manual = w_manual - ALPHA * gradient_w\n",
    "    \n",
    "# Verify:\n",
    "    \n",
    "print(\"Final w_manual:\", w_manual)\n",
    "print(\"Final w_manual_cost:\", w_manual_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "95163d93-73ab-4fed-8cc6-0a464f0c74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b65c2f-022c-41b8-b9ae-a47c3e847e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "# First ALPHA:  .00001\n",
    "# Second ALPHA: .0000000001\n",
    "# Final ALPHA:  .00000001\n",
    "#\n",
    "# I played around with some alpha values and ended up splitting my first and second values. \n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# I could see the curve flattening out after 50 iterations. However, I ran it for 75 iterations to confirm \n",
    "# that the curve was in fact flattening.\n",
    "#\n",
    "# What was your final vector of weights? (w_manual)\n",
    "#\n",
    "# Final w_manual: [-2.0725000e-08  1.0805875e-06 -1.4301625e-06  5.0995000e-07  -2.1075500e-06]\n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost)\n",
    "#\n",
    "# Final w_manual_cost: 0.5544468889068895\n",
    "#\n",
    "# What was your final vector of weights from Part A? (w_direct)\n",
    "#\n",
    "# w_direct: [-0.74210242  0.10016553 -0.63075989  0.06833209 -0.13407739]\n",
    "#\n",
    "# What cost of these weights? (w_direct_cost)\n",
    "#\n",
    "# w__direct_cost: .058819782685208585\n",
    "#\n",
    "# How close are your weights from Part B to the \"correct\" weights from Part A?\n",
    "#\n",
    "# My weights for part B are must smaller values than the weights for part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b703235e-8ccc-4842-8b80-c28b1a8a38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partB and test_acc_partB.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_acc_partB = compute_accuracy(X_train, y_train, w_direct)\n",
    "test_acc_partB = compute_accuracy(X_test, y_test, w_direct)\n",
    "\n",
    "print(train_acc_partB)  \n",
    "print(test_acc_partB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f4c9d1b-8cc9-40b9-b588-4a9ca859b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# How does the accuracy of the model you created by hand in Part B compare to \n",
    "# the accuracy of the model from Part A created by scikit-learn?\n",
    "\n",
    "# ANSWER:\n",
    "# The accuracy of both models were the same. \n",
    "# 0.9625 on the training data\n",
    "# 0.945 on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bdf7cf7e-37dc-4fb5-a08b-876e1140c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "Weights: [-0.74210242  0.10016553 -0.63075989  0.06833209 -0.13407739]\n",
      "Cost: 0.058819782685208585\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n",
      "Part B\n",
      "Weights: [-2.0725000e-08  1.0805875e-06 -1.4301625e-06  5.0995000e-07\n",
      " -2.1075500e-06]\n",
      "Cost: 0.5544468889068895\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", w_direct)\n",
    "print(\"Cost:\", w_direct_cost)\n",
    "print(\"Training accuracy:\", train_acc_partA)\n",
    "print(\"Testing accuracy:\", test_acc_partA)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", w_manual)\n",
    "print(\"Cost:\", w_manual_cost)\n",
    "print(\"Training accuracy:\", train_acc_partB)\n",
    "print(\"Testing accuracy:\", test_acc_partB)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf88d4-1db9-48a1-bca7-44e87b9c2dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
